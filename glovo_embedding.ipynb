{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5080a538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sent2vec.vectorizer import Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8817254d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed = pd.read_csv(filepath_or_buffer='./Dataset/dataframes/df_preprocessed_without_numbers.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "979d7675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  id                     query  median_relevance  \\\n",
      "0           0   1  bridal shower decoration                 1   \n",
      "1           1   2      lead christmas light                 4   \n",
      "2           2   4                 projector                 4   \n",
      "\n",
      "   relevance_variance                                            product  \n",
      "0               0.000  accent pillow heart design red black red satin...  \n",
      "1               0.000  set battery operated multi led train christmas...  \n",
      "2               0.471                 viewsonic dlp multimedia projector  \n"
     ]
    }
   ],
   "source": [
    "print(df_preprocessed.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91342ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = df_preprocessed['query']\n",
    "product = df_preprocessed['product']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "439a5f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed['len_q'] = [len(queries[x].split()) for x in range(0, len(queries))] \n",
    "df_preprocessed['len_p'] = [len(product[x].split()) for x in range(0, len(product))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2072eaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed = df_preprocessed.loc[(df_preprocessed['len_q'] < 512) & (df_preprocessed['len_p'] < 512)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0ce53c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = df_preprocessed['query']\n",
    "product = df_preprocessed['product']\n",
    "classes = df_preprocessed['median_relevance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "05a4dac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10141\n",
      "10141\n",
      "10141\n"
     ]
    }
   ],
   "source": [
    "print(len(queries))\n",
    "print(len(product))\n",
    "print(len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "51a475d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents1 = [x for x in queries]\n",
    "sents2 = [x for x in product]\n",
    "classses = [x for x in classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b57361bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Bert distilbert-base-uncased\n",
      "Vectorization done on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "vectorizer = Vectorizer()\n",
    "vectorizer.run(sents1)\n",
    "vectors = vectorizer.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53c121b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Bert distilbert-base-uncased\n",
      "Vectorization done on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "vectorizer = Vectorizer()\n",
    "vectorizer.run(sents2[:3000])\n",
    "vectors2 = vectorizer.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca6d3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Bert distilbert-base-uncased\n",
      "Vectorization done on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "vectorizer = Vectorizer()\n",
    "vectorizer.run(sents2[3000:])\n",
    "vectors22 = vectorizer.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b851b42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from podium import Vocab, Field, LabelField\n",
    "from podium.datasets import TabularDataset\n",
    "from podium.vectorizers import GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b2a3af58",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame()\n",
    "train['query'] = sents1\n",
    "train['product'] = sents2\n",
    "train['label'] = classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "42dfa6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user2\\anaconda3\\lib\\site-packages\\podium\\vocab.py:514: UserWarning: Vocabulary is finalized already. This should be used only if multiple fields use same vocabulary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "max_vocab_size = 10_000\n",
    "vocab = Vocab(max_size=max_vocab_size, min_freq=2)\n",
    "\n",
    "S1 = Field('query', numericalizer=vocab, pretokenize_hooks=[lowercase])\n",
    "S2 = Field('product', numericalizer=vocab, pretokenize_hooks=[lowercase])\n",
    "LABEL = LabelField('label')\n",
    "\n",
    "fields = [\n",
    "    S1,\n",
    "    S2,\n",
    "    LABEL,\n",
    "]\n",
    "\n",
    "train = TabularDataset.from_pandas(train, fields)\n",
    "train.finalize_fields()\n",
    "\n",
    "glove = GloVe()\n",
    "# Load only the vectors of vocab words.\n",
    "embeddings = glove.load_vocab(vocab)\n",
    "\n",
    "# Generate padded batch.\n",
    "train_batch = train.batch(add_padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02f4966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    \"\"\"\n",
    "    Receives two 2D numpy arrays and calculates cosine similarity across the second axis.\n",
    "    For examples, if `a` and `b` have shape (32, 10), the resulting array should have shape (32,).\n",
    "    \n",
    "    Returns:\n",
    "        1D numpy array with cosine similarities\n",
    "    \"\"\"\n",
    "    return [np.dot(a[i,:],b[i,:])/(np.linalg.norm(a[i,:])*np.linalg.norm(b[i,:])) for i in range(len(a))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bef2d7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10141, 504)\n"
     ]
    }
   ],
   "source": [
    "print(train_batch.product.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6dc6ab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_train = np.array([[embeddings[index] for index in query] for query in train_batch.query])\n",
    "product_train = np.array([[embeddings[index] for index in product] for product in train_batch.product])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9cb0dc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_train_mean = np.array([np.mean(x, axis = 0) for x in query_train])\n",
    "product_train_mean = np.array([np.mean(x, axis = 0) for x in product_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "10af4451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10141, 300)\n",
      "(10141, 300)\n"
     ]
    }
   ],
   "source": [
    "print(query_train_mean.shape)\n",
    "print(product_train_mean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1b35de18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dacd7382",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = query_train_mean.tolist()\n",
    "product_vector = product_train_mean.tolist()\n",
    "labels = np.array(classes).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "882857b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embedding_without_numbers.json', 'w') as outfile:\n",
    "  json.dump([query_vector, product_vector, labels], outfile,indent=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d17bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
